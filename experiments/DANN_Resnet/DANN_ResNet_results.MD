Я провела эксперименты с DANN на ResNet, чтобы понять, насколько лучше он работает, чем DANN на Alexnet, и насколько сопоставимо с результатами из обзора архитектур.
Я обучалась на полном target датасете и считала метрики на валидации по нему же. В таблицу добавляла лучшее значение метрики за время обучения. 

Если разбить target на 0.9/0.1 и выбирать лучшую модель по валидационной части, результаты практически не меняются (проверила на примерах). 

**Обучение baseline**
Я обучила машину OneHeadModel (DANN на ResNet без доменного классификатора) со всеми замороженными слоями и с 129 замороженными слоями, чтобы получить baseline и оценить, насколько добавление доменного loss улучшает результат.

* sgd, lr=0.01, все слои backbone заморожены

![OneDomain_ResNet_frozen](./plots/OneDomain_ResNet_frozen.png)

* sgd, lr=0.01, backbone заморожен до 129 слоя

![OneDomain_ResNet_frozen_129](./plots/OneDomain_ResNet_frozen_129.png)

Модель с размороженными последними слоями стабильно показывает лучшее качество.

**Заморозка слоев**

Сначала просто сравнила, как на доменах Amazon -> DSLR учится модель, и насколько на нее влияет разморозка последних слоев.
Использовала sgd, lr=0.01.

![DANN_with_resnet_amazon_dslr](./plots/DANN_with_resnet_amazon_dslr.png)

Кажестся, что модель с замороженными слоями более стабильна, но модели с размороженными последними слоями учатся гораздо быстрее. 

**Learning rate**

Поняв, что при разморозке слоев модель начинает быстро терять качество на target, хотя loss продолжаает падать, я подумала, что можно поэкспериментировать с learning rate.
![DANN_with_resnet_amazon_dslr](./plots/DANN_with_resnet_amazon_dslr_learning_rate_exp.png)

Learning rate не особо влияет. Или надо было больше его менять.

*Нужно будет еще провести эксперимент с обучением модели с замороженной backbone, разморозкой и дообучением на маленьком lr.*

*И, возможно, попрбовать увеличить вес доменного loss, если первое не поможет.*

**Оценка качества на всех доменах**
Далее я решила проверить качество модели на всех доменах.

* sgd, lr=0.01, все слои backbone заморожены

![DANN_with_resnet_frozen](./plots/DANN_with_resnet_frozen.png)

* sgd, lr=0.01, backbone заморожен до 141 слоя

![DANN_with_resnet_frozen_141](./plots/DANN_with_resnet_frozen_141.png)

* sgd, lr=0.01, backbone заморожен до 129 слоя

![DANN_with_resnet_frozen_129](./plots/DANN_with_resnet_frozen_129.png)

**Rich ResNet**

Я взяла архитектуру классификатора как у AlexNet и использовала ее с ResNet. 
 
![DANN_Resnet_Rich](./plots/DANN_Resnet_Rich.JPG)
Качество уже получилось лучше, чем в обзоре, из которого мы брали данные по DANN на ResNet.

Потом я поэкспериментировала с добавлением BottleNeck меньшего размера перед ветвлением сетки на доменный классификатор и классификатор классов.
Получилось добиться улучшения качества и более стабильного обучения.
![DANN_Resnet_Rich_bottleneck](./plots/DANN_Resnet_Rich_bottleneck.JPG)

Model | A → W | D→ W | W→ D	| A→ D | D → A | W→ A 
--- | --- | --- | --- | --- | --- | --- |
DANN Alexnet Статья | 0.73 |	0.964 | 0.992||||
DANN Resnet Обзор | 0.826 | 0.978 | 1 | 0.833 | 0.668	| 0.661 |
OneDomain ResNet Frozen | 0.67839 |	0.92318 | 0.98884 | 0.71652 | 0.62003 | 0.63388 |
OneDomain ResNet Frozen 129 | 0.74740 |	0.97005 | 0.99777 | 0.76786 | 0.63317 | **0.64453** |
DANN Resnet Frozen | 0.73438 | 0.95573 | 0.99554 | 0.77455 | 0.62749 | 0.63175 |
DANN Resnet Frozen 141 | 0.76172 | 0.97526 | 0.99777 | **0.78125** | 0.65270 | *0.63849* |
DANN Resnet Frozen 129 | 0.76693 | **0.98307** | **1.0** | 0.77679 | **0.65554** | *0.63849* |
DANN Rich ResNet Frozen | 0.74740 |
DANN Rich ResNet Frozen 141 | **0.85677** | | | | | 0.70916 |
DANN Rich ResNet Frozen 129 | 0.85807 |
DANN Rich ResNet Frozen 72 | 0.83464 |
DANN Rich ResNet Frozen 141 Bottleneck 128 | 0.81771 |
DANN Rich ResNet Frozen 141 Bottleneck 256 | **0.86328** | | | | | 0.70881 |
DANN Rich ResNet Frozen 141 Bottleneck 512 | 0.84896 | | | | | 0.70810 |
DANN Rich ResNet Frozen 141 Bottleneck 1024 | **0.87891** | | | | | **0.71626** |
DANN Rich ResNet Frozen 141 Bottleneck 2048 | 0.86068 | | | | |  0.6985 |

**Жирным** выделила лучшие результаты из моих экспериментов. 
`Так` выделила результаты, которые сильно близки (не более, чем на 0.02) к результатам авторов обзора, которые прикрутили ResNet к DANN. 
Наибольшее отставание по качеству для source - amazon.

Разморозка слоев работает, но есть проблема, что качество на target начинает падать, пока качество на source растет. Видимо, если потюнить что-нибудь в loss можно добиться лучшего результата. 
 